# Ollama
Ollama is a way to host AI locally. 
## Installing Ollama
* Command: ```curl -fsSL https://ollama.com/install.sh | sh```
* check http://localhost:11434 for **ollama is running**
## Installing AI models locally
* Command: ```ollama pull <model name>```
    * Example: llama3
* ```ollama run <model name>```
* chat in the terminal
